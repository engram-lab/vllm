name: Latency Benchmark

on:
  push:
    branches: [main]
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  latency-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
      GPU_TYPE: "A100"
      GPU_COUNT: "1"
      TPOT_THRESHOLD_MS: "15.0"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install Modal CLI
        run: |
          python -m pip install --upgrade pip
          pip install modal

      - name: Run latency benchmark without adapters
        run: |
          modal run .github/scripts/modal_ci_benchmark.py \
            --config-paths .github/benchmark_configs/base_latency.json \
            --config-paths .github/benchmark_configs/prefix_latency.json

      - name: Report results
        if: always()
        run: |
          echo "Latency benchmark completed"
          echo "Check the logs above for results"
